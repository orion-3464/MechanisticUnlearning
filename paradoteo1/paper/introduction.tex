\IEEEPARstart{T}{he} rapid development of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP), demonstrating remarkable results in a variety of tasks \cite{forecast5030030}. However, this success is accompanied by concerns regarding data privacy and copyright compliance. Modern LLMs are trained on massive, indiscriminately scraped datasets, leading to the unintended memorization of sensitive information, such as Personally Identifiable Information (PII) and copyright-protected content \cite{carlini2023quantifyingmemorizationneurallanguage}. This memorization process poses legal and ethical risks, particularly when models regurgitate training data during deployment \cite{Lucchi_2024} \cite{10.1145/3593013.3594067} \cite{https://doi.org/10.1002/pra2.1009}. These risks highlight the importance to selectively remove specific knowledge from a model without having to bear the cost retraining it from scratch. Consequently, effective Machine Unlearning (MU) would be a great contribution to safe and moral AI advancement. 

The primary goal of MU is to erase the influence of specific data samples (the "forget set") without degrading the model's performance on the remaining data (the "retain set") or necessitating a computationally prohibitive retraining from scratch \cite{liu2024rethinkingmachineunlearninglarge}. Current state-of-the-art techniques, such as Gradient Ascent and Preference Optimization, attempt to achieve this by maximizing the loss on the target data. However, recent studies suggest that these methods may not result in true erasure. Instead, they often lead to suppression, where the model learns to mask the output while the underlying knowledge remains dormant but retrievable under adversarial prompting or specific internal states \cite{Wen2025Adversarial} \cite{xu2025unlearningisntdeletioninvestigating} \cite{unknown}. 

Mechanistic Interpretability (MI) is an emerging field that seeks to reverse engineer deep learning models, decomposing complex behaviors into understandable parts like features (understandable input properties encoded in representations and activations) and circuits (sub-networks responsible for specific behaviors) \cite{Nanda2022Mechanistic} \cite{rai2025practicalreviewmechanisticinterpretability}.   

This paper is structured as follows. First, I present key concepts of MI establishing a way of analyzing model internals. Second, I survey prominent MU algorithms, techniques and benchmarks. Finally, I propose three experiments on small-scale transformer architecturesâ€”specifically nanoGPT, Pythia-160M, and GPT-2 Small. My goal is to reproduce unlearning results on those models and assess wether these methods achieve genuine knowledge erasure rather than superficial suppression.  