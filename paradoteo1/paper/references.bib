@Article{forecast5030030,
	AUTHOR = {Makridakis, Spyros and Petropoulos, Fotios and Kang, Yanfei},
	TITLE = {Large Language Models: Their Success and Impact},
	JOURNAL = {Forecasting},
	VOLUME = {5},
	YEAR = {2023},
	NUMBER = {3},
	PAGES = {536--549},
	URL = {https://www.mdpi.com/2571-9394/5/3/30},
	ISSN = {2571-9394},
	ABSTRACT = {ChatGPT, a state-of-the-art large language model (LLM), is revolutionizing the AI field by exhibiting humanlike skills in a range of tasks that include understanding and answering natural language questions, translating languages, writing code, passing professional exams, and even composing poetry, among its other abilities. ChatGPT has gained an immense popularity since its launch, amassing 100 million active monthly users in just two months, thereby establishing itself as the fastest-growing consumer application to date. This paper discusses the reasons for its success as well as the future prospects of similar large language models (LLMs), with an emphasis on their potential impact on forecasting, a specialized and domain-specific field. This is achieved by first comparing the correctness of the answers of the standard ChatGPT and a custom one, trained using published papers from a subfield of forecasting where the answers to the questions asked are known, allowing us to determine their correctness compared to those of the two ChatGPT versions. Then, we also compare the responses of the two versions on how judgmental adjustments to the statistical/ML forecasts should be applied by firms to improve their accuracy. The paper concludes by considering the future of LLMs and their impact on all aspects of our life and work, as well as on the field of forecasting specifically. Finally, the conclusion section is generated by ChatGPT, which was provided with a condensed version of this paper and asked to write a four-paragraph conclusion.},
	DOI = {10.3390/forecast5030030}
}

@misc{carlini2023quantifyingmemorizationneurallanguage,
	title={Quantifying Memorization Across Neural Language Models}, 
	author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
	year={2023},
	eprint={2202.07646},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2202.07646}, 
}
    
@inproceedings{10.1145/3593013.3594067,
	author = {Hacker, Philipp and Engel, Andreas and Mauer, Marco},
	title = {Regulating ChatGPT and other Large Generative AI Models},
	year = {2023},
	isbn = {9798400701924},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3593013.3594067},
	doi = {10.1145/3593013.3594067},
	abstract = {Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.},
	booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
	pages = {1112–1123},
	numpages = {12},
	location = {Chicago, IL, USA},
	series = {FAccT '23}
}

@article{https://doi.org/10.1002/pra2.1009,
	author = {Centivany, Alissa},
	title = {Mining, Scraping, Training, Generating: Copyright Implications of Generative AI},
	journal = {Proceedings of the Association for Information Science and Technology},
	volume = {61},
	number = {1},
	pages = {68-79},
	keywords = {Artificial intelligence, Copyright, Generative AI, Information Policy, Technology Ethics},
	doi = {https://doi.org/10.1002/pra2.1009},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/pra2.1009},
	eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.1009},
	abstract = {ABSTRACT Generative AI (GenAI) impacts the ways we create, engage with, and understand creative and intellectual works. These new forms of sociotechnical (inter)action pose challenges for existing legal regimes, ethical frameworks, and social relationships. This research undertakes an in-depth copyright analysis of GenAI based on U.S. law, focusing on its fair use doctrine and conceptions of transformation. This work finds that courts' characterization of uses as primarily either “expressive” or “mediating” is an important, though often implicit, factor in their decisions. Furthermore, while “transformative use” has dominated fair use decisions for the past thirty years, findings from this research suggest that GenAI may usher in a renewed emphasis on the doctrine's market harms element which, in application, may be dispositive with respect to GenAI outputs. This work concludes by offering recommendations aimed at clarifying that the value of copyright arises from social and relational aspects of creative practice and sociotechnical transformation. Arguments and rationales that (over)emphasize atomization and algorithmic decontextualization of the material properties of creative works are unlikely to attend to the underlying purpose of the Act: “[t]o promote the Progress of Science and the useful Arts”.},
	year = {2024}
}

@article{Lucchi_2024, 
	title={ChatGPT: A Case Study on Copyright Challenges for Generative Artificial Intelligence Systems},
	volume={15}, 
	DOI={10.1017/err.2023.59}, 
	number={3}, 
	journal={European Journal of Risk Regulation}, author={Lucchi, Nicola}, 
	year={2024}, 
	pages={602–624}
}

@misc{liu2024rethinkingmachineunlearninglarge,
	title={Rethinking Machine Unlearning for Large Language Models}, 
	author={Sijia Liu and Yuanshun Yao and Jinghan Jia and Stephen Casper and Nathalie Baracaldo and Peter Hase and Yuguang Yao and Chris Yuhao Liu and Xiaojun Xu and Hang Li and Kush R. Varshney and Mohit Bansal and Sanmi Koyejo and Yang Liu},
	year={2024},
	eprint={2402.08787},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2402.08787}, 
}

@misc{Wen2025Adversarial,
	author = {Wen, Jiahe and Zou, Andy and Carlini, Nicholas and Wagner, David},
	title = {Adversarial Prompting of Unlearned Language Models},
	year = {2025},
	note = {Final Project Report, Johns Hopkins University},
	url = {https://www.example-url-where-the-report-is-hosted.edu/report} % Placeholder - Replace with the actual URL if found
}

@misc{xu2025unlearningisntdeletioninvestigating,
	title={Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs}, 
	author={Xiaoyu Xu and Xiang Yue and Yang Liu and Qingqing Ye and Huadi Zheng and Peizhao Hu and Minxin Du and Haibo Hu},
	year={2025},
	eprint={2505.16831},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2505.16831}, 
}

@unknown{unknown,
	author = {Sinha, Yash and Baser, Manit and Mandal, Murari and Divakaran, Dinil Mon and Kankanhalli, Mohan},
	year = {2025},
	month = {06},
	pages = {},
	title = {Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models},
	doi = {10.48550/arXiv.2506.17279}
}

@misc{Nanda2022Mechanistic,
	author = {Nanda, Neel},
	title = {Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases},
	howpublished = {\url{https://www.transformer-circuits.pub/2022/mech-interp-essay}},
	year = {2022},
	note = {Essay available online on transformer-circuits.pub}
}

@misc{rai2025practicalreviewmechanisticinterpretability,
	title={A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models}, 
	author={Daking Rai and Yilun Zhou and Shi Feng and Abulhair Saparov and Ziyu Yao},
	year={2025},
	eprint={2407.02646},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2407.02646}, 
}

@misc{bloom2024saetrainingcodebase,
	title = {SAELens},
	author = {Bloom, Joseph and Tigges, Curt and Duong, Anthony and Chanin, David},
	year = {2024},
	howpublished = {\url{https://github.com/decoderesearch/SAELens}},
}
